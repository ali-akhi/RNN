# -*- coding: utf-8 -*-
"""Untitled25.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nxyC9IEbGY3ViKnXuVKbS1-_DxJHTSNp
"""

# !sudo update-alternatives --config python3
import pandas as pd
import numpy as np
import json
from google.colab import drive
import torch
import torch.nn as nn
import torch.optim as optim

import time
import os
import io
import random
import re
# !pip install torchtext==0.10.0
import torchtext
# !pip install legacy
# from misc import create_supervised_evaluator
# from misc import ModelLoader
# !pip install pytorch-ignite
!pip install pytorch-ignite
# from model import RNN
from pydoc import locate
import csv
# from torchtext.utils import unicode_csv_reader
from torchtext import data
# from torchtext.legacy import data
# from torchtext.legacy import datasets
# from text_cleaning import cleanup_text
# from model import RNN


from torch.nn.parallel import DataParallel

from ignite.engine import Engine
from ignite.handlers import ModelCheckpoint
from ignite.metrics import Accuracy, Precision, Recall, Loss
from ignite.engine import Events
# from torchtext.legacy import data
from ignite.engine import Engine
from ignite.handlers import ModelCheckpoint
from ignite.metrics import Accuracy, Precision, Recall, Loss
from torch.nn.utils.rnn import pack_padded_sequence,pad_packed_sequence

drive.mount('/content/drive')
dataset= '/content/drive/MyDrive/request_label.csv'

#function for read data frame

def read_df(path):
    content=pd.read_csv(path)
    return content

#save csv data frame
def save_csv(extention, name, df):
  full_name= name + '.' + extention
  csv_name= name + '.' + 'csv'
  compression_opts = dict(method=extention,
                        archive_name=csv_name)  
  df.to_csv(full_name, index=False,
          compression=compression_opts)

# dataset= config_data['dataset_full_path']
# neg_labels = int(config_data['num_neg_labels'])
# pos_labels = int(config_data['num_pos_labels'])
# train_filepath = config_data['trainset_fullpath']
# test_filepath = config_data['testset_fullpath']
# training_samples = int(config_data['num_training_sample'])

#const values

num_label= 1000
train_filepath= dataset
train_filepath
training_samples= 1000

#misc
def create_supervised_evaluator(model, inference_fn, metrics={}, cuda=False):
    """
    Factory function for creating an evaluator for supervised models.
    Extended version from ignite's create_supervised_evaluator
    Args:
        model (torch.nn.Module): the model to train
        inference_fn (function): inference function
        metrics (dict of str: Metric): a map of metric names to Metrics
        cuda (bool, optional): whether or not to transfer batch to GPU (default: False)
    Returns:
        Engine: an evaluator engine with supervised inference function
    """

    engine = Engine(inference_fn)

    for name, metric in metrics.items():
        metric.attach(engine, name)

    return engine


class ModelLoader(object):
    def __init__(self, model, dirname, filename_prefix):
        self._dirname = dirname
        self._fname_prefix = filename_prefix
        self._model = model
        self._fname = os.path.join(dirname, filename_prefix)
        self.skip_load = False

        # Ensure model is not None
        if not isinstance(model, nn.Module):
            raise ValueError("model should be an object of nn.Module")

        # Ensure that dirname exists
        if not os.path.exists(dirname):
            self.skip_load = True
            logging.warning(
                "Dir '{}' is not found, skip restoring model".format(dirname)
            )

        if len(glob.glob(self._fname + "_*")) == 0:
            self.skip_load = True
            logging.warning(
                "File '{}-*.pth' is not found, skip restoring model".format(self._fname)
            )

    def _load(self, path):
        if not self.skip_load:
            models = sorted(glob.glob(path))
            latest_model = models[-1]

            try:
                if isinstance(self._model, nn.parallel.DataParallel):
                    self._model.module.load_state_dict(torch.load(latest_model))
                else:
                    self._model.load_state_dict(torch.load(latest_model))
                print("Successfull loading {}!".format(latest_model))
            except Exception as e:
                logging.exception(
                    "Something wrong while restoring the model: %s" % str(e)
                )

    def __call__(self, engine, infix_name):
        path = self._fname + "_" + infix_name + "_*"

        self._load(path=path)

#model
class RNN(nn.Module):
    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):
        super().__init__()

        self.embedding = nn.Embedding(input_dim, embedding_dim)

        self.rnn = nn.RNN(embedding_dim, hidden_dim)

        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, text):
        #forward propagation
        # text = [sent len, batch size]
        # first pass the text into embedding layer
        embedded = self.embedding(text)

        # embedded = [sent len, batch size, emb dim]

        #
        output, hidden = self.rnn(embedded)

        # output = [sent len, batch size, hid dim]
        # hidden = [1, batch size, hid dim]

        assert torch.equal(output[-1, :, :], hidden.squeeze(0))

        return self.fc(hidden.squeeze(0))

def cleanup_text(texts):
    cleaned_text = []
    for text in texts:
        # remove ugly &quot and &amp
        text = re.sub(r"&quot;(.*?)&quot;", "\g<1>", text)
        text = re.sub(r"&amp;", "", text)

        # replace emoticon
        text = re.sub(
            r"(^| )(\:\w+\:|\<[\/\\]?3|[\(\)\\\D|\*\$][\-\^]?[\:\;\=]|[\:\;\=B8][\-\^]?[3DOPp\@\$\*\\\)\(\/\|])(?=\s|[\!\.\?]|$)",
            "\g<1>TOKEMOTICON",
            text,
        )

        text = text.lower()
        text = text.replace("tokemoticon", "TOKEMOTICON")

        # replace url
        text = re.sub(
            r"(http|ftp|https)://([\w_-]+(?:(?:\.[\w_-]+)+))([\w.,@?^=%&:/~+#-]*[\w@?^=%&/~+#-])?",
            "TOKURL",
            text,
        )

        # replace mention
        text = re.sub(r"@[\w]+", "TOKMENTION", text)

        # replace hashtag
        text = re.sub(r"#[\w]+", "TOKHASHTAG", text)

        # replace dollar
        text = re.sub(r"\$\d+", "TOKDOLLAR", text)

        # remove punctuation
        text = re.sub("[^a-zA-Z0-9]", " ", text)

        # remove multiple spaces
        text = re.sub(r" +", " ", text)

        # remove newline
        text = re.sub(r"\n", " ", text)

        cleaned_text.append(text)
    return cleaned_text

class trainer(object):
    def __init__(self,data_path,model_dir,model_name,device=-1):
        self.data_path= data_path
        self.model_dir = model_dir
        self.model_name= model_name
        self.device = device

    @staticmethod
    def train(model, iterator, optimizer, criterion):
        """Train function to start the training the declared  model. model.train() initialize it.
        for every batch picked up from the iterator we send it to the model and get predictions.
        loss = Predicted_y - Actual_y and based of the loss we calculate accuracy.
        loss.backward is for back propagation"""
        epoch_loss = 0
        epoch_acc = 0

        model.train()

        for batch in iterator:
            optimizer.zero_grad()

            predictions = model(batch.sentences[0]).squeeze(1)

            loss = criterion(predictions, batch.labels)

            acc = trainer.binary_accuracy(predictions, batch.labels)

            loss.backward() #back propagation

            optimizer.step() #weight update

            epoch_loss += loss.item()
            epoch_acc += acc.item()

        return epoch_loss / len(iterator), epoch_acc / len(iterator)

    @staticmethod
    def count_parameters(model):
        return sum(p.numel() for p in model.parameters() if p.requires_grad)

    @staticmethod
    def binary_accuracy(preds, y):
        """
        Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8
        """

        # round predictions to the closest integer
        rounded_preds = torch.round(torch.sigmoid(preds))
        correct = (rounded_preds == y).float()  # convert into float for division
        acc = correct.sum() / len(correct)
        return acc

    @staticmethod
    def evaluate(model, iterator, criterion):
        epoch_loss = 0
        epoch_acc = 0

        model.eval()

        with torch.no_grad():
            for batch in iterator:
                predictions = model(batch.sentences[0]).squeeze(1)

                loss = criterion(predictions, batch.labels)

                acc = trainer.binary_accuracy(predictions, batch.labels)

                epoch_loss += loss.item()
                epoch_acc += acc.item()

        return epoch_loss / len(iterator), epoch_acc / len(iterator)

    @staticmethod
    def epoch_time(start_time, end_time):
        elapsed_time = end_time - start_time
        elapsed_mins = int(elapsed_time / 60)
        elapsed_secs = int(elapsed_time - (elapsed_mins * 60))
        return elapsed_mins, elapsed_secs

#Parameters we have provided for our model
EMBEDDING_DIM = 100
HIDDEN_DIM = 256
OUTPUT_DIM = 1
BATCH_SIZE = 512
MAX_VOCAB_SIZE = 25_000


data_path = "/content/drive/MyDrive/"
model_dir = "/content/drive/MyDrive/"
device = -1
model_name = "sentiment_classifer_rnn_sagar.pt"
learning_rate = float(1e-3)
num_epoch = int(20)



################-------##################
Model_trainer = trainer(data_path,model_dir,model_name,device)

# seed
torch.manual_seed(0)
if torch.cuda.is_available():
    torch.cuda.manual_seed(0)
    device = None

tokenize = lambda s: s.split()

text = data.legacy.Field(
    preprocessing=cleanup_text, include_lengths=True, tokenize=tokenize
)

sentiment = data.legacy.LabelField(dtype=torch.float)
train, test = data.TabularDataset.splits(
    Model_trainer.data_path,
    train="train.csv",
    validation="test.csv",
    format="csv",
    fields=[("labels", sentiment), ("sentences", text)],
)

text.build_vocab(train.text, min_freq=1, max_size=MAX_VOCAB_SIZE)
sentiment.build_vocab(train.sentiment)

print(len(train), len(test))

print(vars(train.examples[5]))

train_data, valid_data = train.split(random_state=random.seed(800))
print(f'Number of training examples: {len(train_data)}')
print(f'Number of validation examples: {len(valid_data)}')
print(f'Number of testing examples: {len(test)}')



text.build_vocab(train_data, max_size=MAX_VOCAB_SIZE)
sentiment.build_vocab(train_data)

print(f"Unique tokens in TEXT vocabulary: {len(text.vocab)}")
print(f"Unique tokens in LABEL vocabulary: {len(sentiment.vocab)}")


device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(
    datasets=[train_data, valid_data, test],
    batch_size=BATCH_SIZE,
    sort_within_batch=True,
    sort_key=lambda x: len(x.sentences),
    device=device, )

INPUT_DIM = len(text.vocab)
model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)
print(f'The model has {Model_trainer.count_parameters(model):,} trainable parameters')
model = model.to(device)

optimizer = optim.Adam(model.parameters(), lr=learning_rate)
criterion = nn.BCEWithLogitsLoss()
criterion = criterion.to(device)


N_EPOCHS = num_epoch

best_valid_loss = float('inf')

for epoch in range(N_EPOCHS):

    start_time = time.time()

    train_loss, train_acc =  Model_trainer.train(model, train_iterator, optimizer, criterion)
    valid_loss, valid_acc = Model_trainer.evaluate(model, valid_iterator, criterion)

    end_time = time.time()

    epoch_mins, epoch_secs = Model_trainer.epoch_time(start_time, end_time)

    if valid_loss < best_valid_loss:
        best_valid_loss = valid_loss
        torch.save(model.state_dict(), os.path.join(Model_trainer.model_dir,Model_trainer.model_name))

    print(f'Epoch: {epoch + 1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')
    print(f'\tTrain Loss: {train_loss:.3f} | Train Accuracy: {train_acc * 100:.2f}%')
    print(f'\t Validation Loss: {valid_loss:.3f} |  Validation Accuracy: {valid_acc * 100:.2f}%')


#Loading model from directory and testing : test score and accuracy
model.load_state_dict(torch.load(os.path.join(Model_trainer.model_dir,Model_trainer.model_name)))

test_loss, test_acc = Model_trainer.evaluate(model, test_iterator, criterion)

print(f'Overall Test Loss: {test_loss:.3f} | Overall Test Accuracy: {test_acc * 100:.2f}%')

#read dataset
dataset_df  = pd.read_csv(dataset,names=['labels','id','datetime','query','username','sentences'],header= None,sep=',',encoding = "ISO-8859-1")

# train_test split
train_df= dataset_df[:500]
test_df= dataset_df[500:]

#saving train and test df
save_csv('zip', 'train', train_df)
save_csv('zip', 'test', test_df)